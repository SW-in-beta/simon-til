# ⭐️ 데이터 분석 전에 수행해야 하는 전처리 과정과 데이터 품질을 향상시키는 방법을 설명하시오

## 데이터 전처리(Data Preprocessing)

<aside>
💡

데이터를 분석하기 전에, 원본 데이터를 머신이 사용하는 알고리즘에 적합한 상태로 처리하는 과정.

</aside>

## 왜 데이터 전처리를 해야할까?

<aside>
💡

정확하고 신뢰성 있는 결과를 얻기 위해서는 분석 및 학습에 사용하고자 하는 Model에 맞게 데이터를 처리하는 과정이 필요하다. Garbage in, garbage out!

</aside>

![데이터 분석 과정](https://framerusercontent.com/images/IO1Qh9zhTGWXfv1B8PgG4ulXw3I.png)

데이터 분석 과정

- 현존하는 머신러닝 알고리즘으로는 분석할 수 있는 데이터 유형이 한정적
    - 데이터는 다양한 형식으로 존재하고 있으므로, 항상 원하는 형태로 수집하는 것은 불가
    - 정해진 범위 밖의 값 / 결측치(Missing Value) / 통일되지 않은 형식 등이 존재할 수 있음
- 데이터 사이언스 관점에서, 이러한 이슈는 Insight의 정확도나 신뢰성을 하락시키는 요인
- 머신러닝의 관점에서, 일관적이지 않은 데이터는 학습하는데 많은 시간이 소요되고 자원의 낭비를 초래

## 데이터 전처리에는 어떤 과정이 있나?

!https://framerusercontent.com/images/W45Na0PeEKPJVt5dk35aML4gj98.png

### 1. Data Cleaning

<aside>
💡

결측치를 채워넣고, 노이즈 데이터를 완화하는 등 분석에 방해되는 것들을 깨끗하게 하는 과정

</aside>

- 결측치(Missing Value)
    1. 결측치 제거 : 해당 데이터를 제거하는 방법. 데이터셋이 방대하고, 결측치 또한 많을 때 고려된다.
    2. 결측치 대체 : 해당 결측치에 들어갈 값을 예상하여 채워넣는 방법. 평균이나 회귀 등을 활용한다.
- 노이즈(Noisy Data)
    1. Binning : 정렬된 데이터를 일정한 간격의 그룹으로 묶어, 해당 그룹 내의 데이터를 평균 / 중앙값 등으로 대체하는 방법. Downsampling이라고 생각하면 된다.
    2. Regression : 모든 데이터로부터 Regression Function을 구하고, 각 데이터를 Regression Function의 결과값으로 대체하는 방법.
        
        ![Regression Line](https://d1whtlypfis84e.cloudfront.net/guides/wp-content/uploads/2019/05/24071439/Regression-lines.png)
        
        Regression Line
        
    3. Clustering : 비슷한 값을 갖는 데이터로부터 Cluster를 형성하는 방법. Cluster 안에 속하지 않은 데이터는 노이즈라고 간주하고 삭제할 수 있다.

### 2. Data Integration

<aside>
💡

여러 출처로 부터 수집된 데이터를  통합하여 하나의 큰 저장소에 저장하는 과정

</aside>

- Schema Integration and Object Matching
    - 데이터는 각각 다른 포맷으로 존재하거나 다른 특성들을 지니므로 사전에 통합하는 것이 필요
- Removing Redundant Attributes
    - 통합된 데이터에서 중복된 특성들을 제거
- Detection and Resolution of Data Value Conflicts.

### 3. Data Transformation

<aside>
💡

통합된 데이터의 값, 구조, 포맷 등을 수정하여 고품질의 데이터로 변환하는 과정

</aside>

- Generalization
    - 하위 개념의 정보를 상위 개념의 정보로 변환하는 방법.
        - city(하위 개념) 주소를 country(상위 개념) 주소로 변환
- Normalization
    - 수치 데이터를 특정 범위의 값으로 변환하는 방법.
    - 다른 범주를 갖는 데이터의 관계성을 분석하기 위해 같은 수치로 변환하는 등 특정 분석을 수행하기 용이한 값으로 변환
- Attribute Selection
    - 기존의 데이터로부터 새로운 특성을 뽑아내는 방법.
        - ‘생년월일’ 데이터로 부터 ‘고령자’, ‘청년’ 등의 특성을 뽑아 변환
- Aggregation
    - 기존의 데이터를 요약된 포맷으로 저장 / 표현하는 방법.
        - 일 매출 데이터를 모아 월 / 연단위 매출로 변환

### 4. Data Reduction

<aside>
💡

Dataset의 크기가 너무 클 때, 더 작은 크기의 요약된 표현으로 데이터를 축소 변환하는 과정

</aside>

- Dimensionality Reduction
    - 데이터가 갖는 Feature(ex. 각각의 column) 중 비슷한 특징을 갖는 Feature를 제거 / 통합하여 차원의 수를 축소하는 방법
        - PCA(Principal Component Analysis) 등을 활용하여 분석 알고리즘에 필요한 Feature만 추출
- Data Compression
    - Encoding 기술 등을 활용하여 데이터를 압축하는 방법
    - 데이터 손실 여부에 따라 Lossy / Lossless Reduction으로 분류
- Discretization
    - 연속형 데이터를 유의미한 구간으로 분류된 데이터로 나누어 축소하는 방법
    - 연속형 데이터는 그 자체로 분석의 대상이 되기 힘든 경우가 많음
        - ‘나이’ 데이터를 18세 이하, 18-44세, 44-60세, 60세 이상 등으로 분류하면 같은 구간의 데이터를 분석 대상으로 간주할 수 있음
- Numerosity reduction
    - 다른 Feature로 부터 도출될 수 있는 경우 등의 Feature는 dataset에서 제거하는 방법
        - 특정 등식을 통해 쉽게 알 수 있는 데이터(ex. Regression Model)는 결과값 대신 해당 등식만 저장

## 품질이 좋은 데이터는 어떤 데이터인가?

<aside>
💡

정확하고 일관성있는, 그리고 활용 목적에 맞게 적절히 수집된 데이터

</aside>

- Data Quality
    - 정보의 양적 / 질적인 상태를 의미
    - 사용되는 의도에 적합하고, 실제 세계의 정보를 잘 표현한 데이터가 고품질 데이터
    - 데이터의 양이 증가함에 따라, 데이터가 일관성을 갖는지도 중요
- Data Quality Assessment
    - 데이터에 문제가 없는지 통계적 방식으로 접근하여 평가
        1. 결측치가 있는가?
        2. 정확하고 신뢰성 있는 정보인가?(Accuracy and Reliability) → 정보의 옳고 그름에 Focus
        3. 모든 Feature가 일관적인가(Consistency)? → 한 사람이 두 개의 생일을 갖는 경우 등
        4. 유효한 데이터인가?(Validity) → 생년월일이 12월 32일인 경우, cm으로 입력되어야 하는 조건이 있는데 m 단위로 입력된 경우 등
        5. 중복된 데이터가 있는가?
- Data Quality Assurance
    - 데이터의 질을 보장하기 위해 주로 3가지 단계를 수행
        1. Data Profiling : 통계량(min, max, mean 등)과 메타데이터를 중심으로 데이터의 구조를 파악하여 문제가 있는지 분석하는 과정
        2. Data Cleaning : Data Profiling에서 도출된 문제를 보완하는 과정
        3. Data Monitoring : 데이터를 정제된 상태로 유지하고 꾸준히 점검하는 과정

## 어떻게 데이터의 품질을 향상시킬 수 있을까?

<aside>
💡

이제껏 공부한 데이터 전처리만 잘해도 데이터 품질은 올라간다!

</aside>

- 결측치, 이상치, 중복된 데이터 등을 적절히 처리하여 데이터를 정제된 상태로 유지 → **Cleaning**
    - 고객의 구매 금액이 일반적으로 10만원~1백만원 사이인데 10억원의 구매자가 있는 경우
    - 설문 조사 데이터에서 나이 항목이 누락된 응답이 다수 있는 경우
- 데이터가 특정 형식이나 범위를 만족하는지, 다른 테이블 간에 논리적 모순이 없는지 확인 → **Validity**
    - 생년월일이 12월 32일이거나 1870년생인 경우
    - 전화번호 형식이 000-0000-0000으로 지정되어 있는데 01012345678이 있는 경우
- 단위, 형식 등을 일관되게 통일 → **Standardization**
    - 사람의 키는 ‘m’, 발 길이는 ‘mm’ 단위로 저장할 때, 경우에 따라 통일시켜주는 것이 필요할 수 있음
- 다각도로 분석하여 새로운 인사이트를 도출하기 위해 여러 출처의 데이터를 통합하여 유지 → **Integration**
    - SNS 채널에서 수집된 해당 제품에 대한 좋아요 수 + 쇼핑몰 페이지에서 실제 구매 이력 데이터
- 가이드라인 등을 마련하여 지속적으로 데이터를 관리 → **Governance**