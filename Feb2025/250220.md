해커톤에 쓰일 만한 것들을 한 번씩 해보고 있다.
해보면서 느끼는건 역시 안해본건 있어도 못하는건 없는 것 같다.
다만 잘하는거랑 할줄 아는 것은 분명한 차이가 있으니까 익숙해지도록 자주 사용해봐야지 ~


### FAST API

- 사용 예시를 보니 이해가 됐다. 예전에 작은 서버를 구축해봤던게 도움이 된 것 같다.
- API를 설계하려고 하는데, /predict/{operator} 에 따라 뒤에 인자의 개수를 달리하고 싶어서 알아보니 APIRouter라는 것이 있었다.
  - 사용할 일이 많을 듯
  ```python
  predict_router = APIRouter(prefix="/predict")

  @predict_router.get("/{operation}/left/{left}/right/{right}")
  def predict_binary(operation: BinaryOperation, left: int, right: int):
    model = get_model(operation)
    result = predict(model, [left, right])
    return {"result": result}

  @predict_router.get("/{operation}/{value}")
   def predict_unary(operation: UnaryOperation, value: int):
    model = get_model(operation)
    result = predict(model, [value])
    return {"result": result}

  app.include_router(predict_router)
  ```
- 로컬에서 fastapi dev main.py로 돌리고 ngrok으로 연결하니 잘 된다. 그렇지만 내 맥북을 계속 돌릴 수는 없으니 AWS에서 돌리기로 결정

### AWS
- 처음 써봤다. EC2로 t2micro인가 free tier짜리로 해서 어찌어찌 인스턴스를 생성했다.
- 아주 허허벌판의 환경이었다. 다행히 python은 있더라.
- git을 다운받고 내 repository를 clone하는 것까지는 성공했는데.. numpy도 없고 torch도 없고...
  - numpy 설치까지는 금방 했다. 근데 torch를 받다가 killed 한 5번 정도 당하고 나서 포기
  - 이래서 사람들이 docker docker 하는구나 싶다. 그래서 다음 체험은 docker로 결정

### torch로 XOR Model 만들기
- 층을 2겹쌓고, 활성함수는 ReLU와 Sigmoid로 했다.
- epoch는 1000으로 했는데, 어떨 때는 손실값이 계속 유지되고.. 한 10번에 1~2번 꼴로 학습 성공하는 느낌
- 그래서 reset함수를 따로 만들고 retraining을 시키도록 했다. 학습이 안되면 아예 다시 하는걸로
- 다 학습 시킨 모델은 torch.save(self.state_dict(), 주소) 해서 내보냈고, 새로 생성할 때 해당 모델에 대한 state_dict가 있으면 굳이 새로 트레이닝 안하게 설정했다.
- 잘돌아간다 ~~~