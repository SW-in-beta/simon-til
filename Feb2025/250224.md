# ⭐️ 퍼셉트론의 작동 원리와 이진 분류 문제 해결 과정을 설명하시오.

## 퍼셉트론(Perceptron)

<aside>
💡

이진 분류를 해결하기 위한 지도학습 알고리즘.

</aside>

- 이진 분류(Binary Classify) : 벡터로 주어진 Input의 True or False를 판별하는 것
- 지도 학습(Supervised Learning) : 학습 과정에서 학습 데이터와 함께 레이블(주어진 학습 데이터가 True인지 False인지)을 함께 주어 학습시키는 방법

## 퍼셉트론은 어떻게 동작하는가

<aside>
💡

$f(i) = h(w\cdot i + b)$ (그러나 일반적으로 i에 1, w에 b를 추가하여 $f(i) = h(w\cdot i)$로 표현)

$w(weights): vector,\ i(input): vector,\ b(bias): scalar,\ h: Heaviside\ step-func$   

</aside>

- Input : 예측 또는 학습을 하고자 하는 vector.
- weights : input을 분류하기 위해 Input에 곱해지는 벡터. 주 학습 대상
- bias : 결정 경계의 위치를 이동하기 위해 더해지는 스칼라
- Heaviside step-function : [사진](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/640px-Dirac_distribution_CDF.svg.png)
- 학습 및 추론 과정을 이해하기 위한 예시
    - AND 연산의 진리표:
        
        
        | x1 | x2 | AND(출력) |
        | --- | --- | --- |
        | 0 | 0 | 0 |
        | 0 | 1 | 0 |
        | 1 | 0 | 0 |
        | 1 | 1 | 1 |
    - Inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
    - Labels = [0, 0, 0, 1]
    - weights = (w1, w2), bias = b
        
        ![AND Inputs의 분포](attachment:257c07c3-6c4f-4c15-b0cd-2eb530e761e3:Figure_1.png)
        
        AND Inputs의 분포
        
        ![무작위로 설정한 weight(기울기)와 bias(y절편)로 그은 선. 실제로는 저렇지 않지만.. 이해를 돕기 위해 차원을 낮춰 표현했다.](attachment:1d1957b6-977f-41dc-9bbc-344eb62f6bcc:Figure_2.png)
        
        무작위로 설정한 weight(기울기)와 bias(y절편)로 그은 선. 실제로는 저렇지 않지만.. 이해를 돕기 위해 차원을 낮춰 표현했다.
        
    - 빨간 선 위에 있는 x = (x1, x2)가 wx + b = 0을 만족하는 x의 집합
    모든 점들이 wx + b < 0 영역에 있다 ⇒ heaviside step-function의 결과가 전부 0
    - (1, 1)의 결과는 1이어야 하므로 현재는 만족스러운 결과가 아니다
    ⇒ w와 b를 조정하여 잘 분류할 수 있게 학습해야한다.
    - 결정 경계(Decision Boundary) : wx + b가 형성하는 공간
        
        ![학습이 완료된 그래프! wx + b의 값이 0보다 크면 1, 0보다 작으면 0이라고 판단할 수 있다.](attachment:9e649589-9146-4626-98cf-88e373d13333:Figure_3.png)
        
        학습이 완료된 그래프! wx + b의 값이 0보다 크면 1, 0보다 작으면 0이라고 판단할 수 있다.
        

## 퍼셉트론을 어떻게 학습하는가

<aside>
💡

훈련용 데이터(Vector i의 모음)를, w와 b가 임의로 설정된 $f(i) = h(w\cdot i + b)$에 넣어 얻은 값과 비교하며 정확히 분류할 때까지 w와 b를 수정한다.

</aside>

![훈련용 데이터를 2개(1번) → 3개(2번) → 5개(3번) → 6개(4번) 넣을 때 마다 변화하는 결정경계의 모습](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/1920px-Perceptron_example.svg.png)

훈련용 데이터를 2개(1번) → 3개(2번) → 5개(3번) → 6개(4번) 넣을 때 마다 변화하는 결정경계의 모습

1. Input vector $x_n$에 대해 기대하는 결과가 $y_n$이라고 하고, 현재의 weights vector를 **$w_n$**, bias를 $b_n$라고 하자. $r$은 학습률(Learning Rate)이다.
2. 퍼셉트론에 $x_n$를 input으로 주어 얻은 결과를 $x_n$$y' = h (w_n \cdot x_n + b_n)$라고 했을 때,
    1. $y_n = y'_n$라면 $w_n, b_n$을 수정하지 않고 다음 $x_{n+1}$에 대한 학습으로 넘어간다.
    2. $y_n \not= y'$라면 $w_{n+1} = w_n + r \times (y_n - y'_n)x_n, b_{n+1} = b_n + r \times (y_n - y'_n)$

## 그래서 이진 분류 문제는 어떻게 해결하나요

<aside>
💡

학습된 $w, b$를 바탕으로, 새로운 입력에 대해 1 또는 0의 값을 반환하여 분류한다!

</aside>

- $wx + b$가 형성하는 선형 결정 경계를 기준으로 서로 같은 공간에 위치한 입력을 같은 값으로 반환한다.
- ‘선형 결정 경계’다 보니, 비선형 문제(ex. XOR)는 해결할 수 없다.
⇒ 비선형 문제를 해결하려면 다층 퍼셉트론(Multi-Layer Perceptron)이 필요하다.

## 다층 퍼셉트론(MLP)이란

 

<aside>
💡

입력층, 은닉층, 출력층으로 구성되어 있는 여러 층으로 구성된 퍼셉트론 집합.

인접한 층끼리 연결되어 있고, 활성화함수를 사용하여 복잡한 패턴을 학습할 수 있다.

</aside>

![은닉층이 3층인 다층 퍼셉트론](https://hwk0702.github.io//img/mlp.png)

은닉층이 3층인 다층 퍼셉트론

- 입력층 : 모델에 입력되는 데이터를 받는 층
- 은닉층 : 층 별로 여러 개의 퍼셉트론으로 구성되며, 입력 데이터를 가중치와 활성화 함수를 이용해 변환
    - n번째 레이어의 퍼셉트론의 개수가 k개라고 하면, n+1번째 레이어의 퍼셉트론들은 각각 크기가 k인 벡터를 입력받게 된다. (n번째 레이어의 퍼셉트론 k개가 반환한 스칼라 값들을 벡터로)
    ⇒ n+1번째 레이어의 퍼셉트론들은 크기가 k인 벡터를 weights로 갖는다.
- 출력층 : 최종 예측값을 출력하는 층

## 비선형 분류가 뭐길래

<aside>
💡

단순한 직선이나 평면으로 분류할 수 없는 복잡한 패턴을 분류

</aside>

![선형 분류(A)와 비선형 분류(B)](attachment:d479f945-8bf9-4cf7-8305-600c7047d0e0:image.png)

선형 분류(A)와 비선형 분류(B)

## 활성화 함수는 어떻게 비선형성을 추가하는가

<aside>
💡

선형변환된 $wx + b$가 활성화 함수 f를 거치면 비선형 결과인 $f(wx + b)$를 얻을 수 있다.

</aside>

![wx+b로 선형변환하여 얻은 그래프(좌측)와, Sigmoid 함수를 통해 비선형변환한 그래프(우측)](attachment:92d3223a-7867-4a36-9d43-3ebf7a69aa4e:Figure_4.png)

wx+b로 선형변환하여 얻은 그래프(좌측)와, Sigmoid 함수를 통해 비선형변환한 그래프(우측)

## 활성화 함수의 종류는 어떤 것이 있고 어떻게 쓰일까

1. 시그모이드(Sigmoid) 함수
    
    $$
    σ(x)=\dfrac{1}{1+e^{−x}}
    $$
    
    - 출력 범위: (0, 1)
    - 입력을 확률로 변환하는 효과
    - 이진 분류 문제의 출력층에 사용
    - 출력값이 0 또는 1에 수렴할 경우 기울기가 너무 작아져 학습이 어려움(기울기 소실)
    
2. 하이퍼볼릭 탄젠트(Tanh) 함수
    
    $$
    tanh(x)=\dfrac{e^x - e^{-x}}{e^x + e^{-x}}
    $$
    
    - 출력 범위: (-1, 1)
    - 시그모이드와 유사하지만 출력 값이 조정
    - RNN의 은닉층에 사용
    - 중심이 0이므로 시그모이드보다 신경망 학습이 빠르지만 여전히 기울기 소실 존재

1. ReLU(Rectified Linear Unit) 함수
    
    $$
    f(x) = max(0, x)
    $$
    
    - 출력 범위: [0, +oo)
    - 양수 값은 그대로 유지하고 음수는 0
    - 딥러닝 모델에서 기본 활성화 함수
    - 음수 영역에서 기울기가 0이 되므로, Dying ReLU 발생 가능
    ⇒ 단점을 보완하기 위해 음수값에서도 작은 기울기를 유지한 Leaky ReLU 사용
    

1. 소프트맥스(Softmax) 함수
    
    $$
    σ(x_i)=\dfrac{e^{x_i}}{∑_je^{x_j}}
    $$
    
    - 여러 개의 출력을 확률 값으로 변환하는 함수
    - 출력값의 합은 항상 1
    - 다중 클래스를 분류하는 출력층에서 사용

![시그모이드 함수](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/440px-Logistic-curve.svg.png)

시그모이드 함수

![하이퍼볼릭 탄젠트 함수](https://mlnotebook.github.io/img/transferFunctions/tanh.png)

하이퍼볼릭 탄젠트 함수

![ReLU 함수](https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png)

ReLU 함수

![ReLU함수를 보완한 Leaky ReLU함수](https://www.i2tutorials.com/wp-content/media/2019/09/Deep-learning-25-i2tutorials.png)

ReLU함수를 보완한 Leaky ReLU함수

## 다중 퍼셉트론은 어떻게 학습할까

<aside>
💡

순전파 → 손실 계산 → 역전파 & 경사하강법

</aside>

1. **파라미터 초기화** : 각 Layer에 있는 Perceptron의 $w, b$ 를 작은 랜덤 값으로 설정.
2. **순전파(Forward Propagation)** : 입력 → 은닉층 → 출력층으로 진행.
3. **손실 함수 계산**: 예측 값과 실제 값의 차이를 측정.
4. **역전파(Backpropagation)**: 출력층에서 입력층 방향으로 $w, b$의 그래디언트 계산하고, 경사하강법을 사용하여 $w, b$ 조정
5. **반복**: 손실 값이 충분히 작아질 때까지 위 과정 반복.

<aside>
❓

왜 굳이 출력층에서 입력층 방향으로 그래디언트를 계산(Backpropagation)하나요

⇒ 출력층의 그래디언트를 계산하고, 그 값으로 은닉층의 그래디언트를 계산할 수 있습니다!(연쇄법칙)

- 손실함수를 최소화하는 $w, b$ 를 찾는 과정 → 손실함수에 직접적으로 영향을 주는 것은 출력층의 $w, b$
    - 은닉층의 $w, b$가 손실함수의 미치는 영향을 바로 구할 수 없음
- 출력층이 손실함수에 미치는 영향(그래디언트)를 먼저 계산하고, 출력층 바로 직전에 있는 은닉층의 layer가 출력층에 미치는 영향(그래디언트)를 계산하고, … 연쇄적으로 계산
</aside>