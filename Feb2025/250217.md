# 다변수 함수의 미분

### 미분
- 평균변화율의 극한 == 순간 변화율 == 미분계수
  - 변화량 : 기준으로부터 변화까지의 차이 
  - 평균 변화율 : y변화량 / x변화량
- 미분한다 == 미분계수를 구한다 == 도함수를 구한다

### 다변수 함수의 미분
- 일변수 함수에서의 미분 계수는 실수(R) -> 다변수 함수에서의 미분 계수는 함수(선형 변환)⭐️
  - 다변수 함수 f: R^n -> R^m 이면, 미분 계수 L: R^n -> R^m
  - 다변수에서의 변화율은 y변화량 Norm / x변화량 Norm
- 미분 행렬 == 야코비 행렬 == 자코비안 : |L| = Df(x_0)x
  - x_0에서의 선형근사 : M(x) = f(x_0) + L(x - x_0) = f(x_0) + Df(x_0)(x - x_0) -> 일변수 함수에서의 접선 방정식 구하기와 크게 다르지 않다
- 일변수 함수에서는 극한을 할 때 변화량이 스칼라 1과 0으로 가는 변수를 곱하면 되지만, 다변수 함수는?
  - Norm이 1인 벡터를 하면 된다. 그럼 수많은 Norm 1 벡터 중에서 무엇을? -> 표준 기저벡터?
- 그래디언트: 다변수일때의 기울기
  - f(x, y) = x^2 + y^2에서 del f / del x = 2x, del f / del y = 2y => (2x, 2y)가 그래디언트 !
  - 왠지 학습 시 파라미터 조정할 때 쓰일 것 같다.
- 머리 속에서 미분이 어떤 의미인지, 뭘 구하려는 것인지 떠올려보면 더 간단해진다
  - 각 벡터값의 변화량 (x - x_0)에다가 각 벡터값의 변화율(그래디언트)를 내적(끼리끼리 곱해줘서 합)해주면 전체의 순간 변화율이 나오는 것
  - 그리고 이것은 행렬로 표현하면 더 간단해진다!

### 선형 회귀
- 일변수 / 다변수 선형회귀 개념은 아니까 패스. Loss Function이 최소되는 weights + biases 찾기
  - 계산을 하려면 너무... 힘드니까 컴퓨터한테 시키자
  - 컴퓨터도 힘들어하니까... 다른 방법을 찾자 => ex. 경사하강법
- 다변수 경사하강법 => 그래디언트 방향으로 가면 가장 빠르게 증가한다는 개념을 기반으로 일변수와 같다!


# 이미지 생성 모델 트레이닝

### Finetune Flux.1 model with LoRA
- 결과가 만족스러웠다고는 할 수 없지만, 재밌었다.
  - Imaga Generation은 아직 one shot 만으로 만족스러운 결과를 뽑아내긴 어려운 것 같다.
- 같은 프롬프트를 입력해도 다양한 사진이 나왔는데, Diffusion Model의 특성이 여기서 드러나는구나 싶었다.
  - 예전에 한창 Diffusion Model을 Text Generation에 도입하려는 논문들을 읽은 적 있다. 그때는 많이 발전하고 있지만 한계가 있다는게 주론이었는데 지금은 어떤지 찾아봐야겠다.
- LoRA에 대해 얕게 알고 있어서 확실하진 않지만, 내가 이해한대로 설명하자면 기존 트레이닝 모델에 Magic(Trigger) word에 대한 내용을 학습시키는 방식으로 자원을 매우 아낄 수 있다는 장점이 있음
  - 이번에 내 고양이(두식) 사진을 'lovelydoosik'으로 학습시켰는데, 좀 더 유니크한 단어를 사용하면 다 나은 결과가 나오지 않았을까 싶다.
- FineTuning 방식도 굉장히 많이 있겠지만, GPU-rentals-as-a-service인 Replicate에 올라와있는 ostris/flux-dev-lora-trainer를 그대로 사용했다.
  - 사실 다른 분이 하신 것을 보고 그대로 따라한 것이기 때문에 나에게 선택지는 없었다.
  - 참고한 페이지 (https://www.coryzue.com/writing/make-ai-pictures-of-yourself/)
- 내 최근 사진에서 두식이를 안고 있는 사진을 생성하고 싶었는데, 그러한 Inpainting 방식 모델을 학습하는 것은 더욱 힘들다고 해서 일단은 여기까지만 해봤다.
- 이미지 생성 모델은 여기까지하고, 당장 해커톤에 쓸만한 DeepSeek R1이나 GPT Finetuning이나 알아보자