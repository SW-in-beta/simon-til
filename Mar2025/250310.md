### NLP(Natural Language Processing)
- AI와 상관없이 원래 있던 분야다.
    - 그러다 AI가 발전하면서 BOOM
- NLP에서의 표준화와 정규화 : 둘다 Normalization이다.
    - 표준화 : 텍스트 형식을 일정하게 맞추는 작업. ex. u -> you
    - 정규화 : 숫자 데이터를 조정하여 특정범위로 맞추는 작업
- 자연어가 들어왔을 때 어떻게 분석을 하나? -> 벡터화!
    1. BoW(Bag of Words) : 문서나 문장에 어떤 단어가 몇 번 등장했는지로 비교하는 방법
    2. TF-IDF(Term Frequency - Inverse Document Frequency) : 단어의 빈도(TF) + 해당 단어가 다른 문서에 등장하는 빈도(IDF)를 결합해 단어의 중요도를 측정하는 방법
        - 다른 문서에서도 자주 언급된 단어라면 가치가 없다고 판단. ex. the, a ...
- 벡터화를 하고나면? -> 다항 나이브 베이즈!(Multinomial Naive Bayes)
    - 베이즈 정리 : 사전 확률에 새 증거를 반영하여 더 정확한 확률을 구하는 원리 -> 이건 더 공부해보자.
        - 병에 걸릴 확률 1%(사전 확률) -> P(A)
        - 어떤 검사에서 양성이 나왔을 때(증거)
        - 이 병에 걸렸을 때(사후 확률) -> P(A | B)
        - 검사 결과가 양성일 때 실제로 병에 걸렸을 확률(우도) -> P(B | A)
        - 검사 자체가 양성이 나올 확률(정규화 상수) -> P(B)
        - 즉. 병에 걸릴 확률과 검사 결과가 양성이 나올 확률을 알고 있고, 실제 병에 걸렸을 때 검사 결과가 양성이 나올 확률을 알고 있다. 이 때, 어떤 검사에서 양성이 나왔을 때, 병에 걸렸을 확률을 구하는 방법.
        - 검사에서 양성이 나왔을 때 병에 걸렸을 확률 = 병에 걸렸을 때 검사에서 양성이 나왔을 확률 * 병에 걸렸을 확률 / 검사에서 양성이 나왔을 확률
        - 스팸의 경우, 그냥 스팸을 분류하는 확률이 있다. 이걸로 충분하지 않으니까, 어떤 단어가 나왔을 때 스팸일 확률로 보정하는 것이라 보면 될 듯.


### RNN
- 자연어를 수치로 만드는 것까지는 성공했다. 그다음은?
- Seq2seq 방식 -> Sequence가 길어질수록 성능이 떨어졌다. 맥락이 없어서.
- RNN : 순차적인 데이터(텍스트, 시계열 데이터 등) 처리를 위해 설계된 인공 신경망. Linear, Convolusion, Pooling과 같이 Layer라고 보면 된다.
- 어떻게? 과거의 상태를 저장하여 현재의 입력과 결합!
    - I work at home
    - I가 첫째 layer에 들어가면, '대명사일 가능성이 높다'는 정보가 다음 layer로 같이 들어간다.
    - work이 들어갔을 때, I가 대명사일 가능성이 높다는 정보를 가지고 있으니 동사로 판단할 가능성이 커진다! 단순히 work만 들어왔을 때는 이게 명사인지 동사인지 알기 힘들다.
    - 근데 이게 어떻게 실제로 되는지...? 각 Layer에 단어가 하나씩 들어가는지, 아니면 한 Layer에 단어가 순서대로 들어가면서 hidden state가 업데이트 되는 것인지... 한 Layer가 아니고 그냥 Hidden Layer 전체인가? -> 아마 Hidden Layer 전체가 맞는듯
    - 예제 코드를 보면서 자세히 살펴보자.
- 현재 주류 기술은 아니다. 현재는 Transformer가 주류인데, 그 과정에서 살펴가는 기술 정도로 생각하자.


### LSTM(Long Short-Term memory)
- RNN이 체인이 길어지면, 너무 많은 정보를 기억하게 된다.
- RNN에서 출력과 멀리 있는 정보를 기억할 수 없다는 단점을 보완 -> 장/단기 기억을 가능하게 설계한 신경망 구조
- 기본적으로는 RNN 베이스. Gate를 도입해서 정보를 기억할지 계속 유지할지 결정하는 단계를 추가 -> Layer에 Perceptron과 더불어서 Cell을 둔다. Cell에서 Cell로 전달되면서 기억할 State와 기억하지 않을 State를 업데이트
- 그래도 여전히 문제점은 존재한다. 번역은 참 잘하는데 평범한 대화는 좀 어려웠다...


### Transformer ⭐️⭐️⭐️
- 이제껏 모델들은 순서대로 입력을 받아서 생성하므로 병렬 처리를 하지 못했다. 즉, I -> work -> at 이런식으로 하나하나 들어가야 했다. 직전 정보에 의존했다.
- 핵심은 병렬처리 + Self Attention 메커니즘! -> 덕분에 빠르고 정확한 학습이 가능한 자연어 처리 모델이 등장했다.
- 이때까지 배운 모델을은 Prediction이나 Classification이었다면, 이제는 Generation까지 왔다. -> 그러나 크게 벗어난건 아니다.
- 병렬처리 : 위치 인코딩이 바탕이 되었다. 단어의 위치를 단어의 속성의 하나로 보았다!