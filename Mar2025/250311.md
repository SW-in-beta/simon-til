### Feed Forward Neural Network Model(NNLM)
- 기존 N-Gram 방식 : 해당 단어 앞 n-1개 단어와 함께 언급된 빈도로 예측
    - P(w|boy is spreading) = count(boy is spreading w) / count(boy is spreading) -> boy is spreading이 왔을 때, 그 다음 단어 w가 나올 확률은 사전에 boy is spreading w가 나온 빈도로 예측을 한다.
    - 한계 : Sparsity Problem(희소 문제)! 만약 '살펴보다'에 대해 학습한 상태라도, 유사한 의미지만 처음 등장하는 '톺아보다'가 나오면 그전에 시퀀스가 없었으므로 예측에 고려할 수 없다.
    - 아무리 Boy is spreading smile이 있을 수 있는 표현이라고 해도, 학습한 단어 시퀀스에 없었다면 예측할 수 없다는 한계 -> 유사성을 파악하는 것이 필요하다.
- 워드 임베딩 : 기계가 단어를 인식할 수 있게 모든 단어를 벡터화. 유사성 파악에도 용이하다.
    - "what will the fat cat sit on"이라는 문장이 있다고 하자.
        1. 각 단어들을 원-핫 인코딩 한다. ex) what = [1, 0, 0, 0, 0, 0, 0], will = [0, 1, 0, 0, 0, 0, 0] ....
        1-1. 이렇게 원-핫 인코딩한 내용만으로는 단어간 유사도를 파악할 수 없을 뿐 아니라, 문장의 길이가 길어지면 낭비되는 메모리가 너무 많다.
        1-2. 원-핫 인코딩된 벡터들 중에서도 예측하고자 하는 단어의 앞에 있는 n개의 단어 벡터만 사용한다. (여기서 n을 윈도우라고 한다.)
        2. 단어의 개수 * 특정 크기(M)를 갖는 투사층과 원-핫 인코딩한 단어들을 내적한다
        2-1. 이는 딱히 내적값을 구하고자 하는 것이 아니라, 위의 예시에서 what의 경우는 투사층의 0번째 row, will은 투사층의 1번째 row를 가져오는 것에 지나지 않는다. -> 그래서 Lookup Table이라고 한다.
        2-2. 만약 M이 5라면, 내적 결과 what은 [w_0,0, w_0,1, w_0,2, w_0,3, w_0,4]가 되고, will은 [w_1,0, w_1,1, w_1,2, w_1,3, w_1,4]가 된다. -> 결국 이 w들을 학습하는 과정! 유사한 단어끼리는 내적을 했을 때 값이 높게 나오게!
        2-3. 거기에 더해서, 크기가 단어의 길이만큼이었던 벡터를 5로 줄일 수 있는 효과도 있다.
        3. 투사층은 활성화함수를 거치지 않는다. 그냥 단어를 벡터화한다는 것에 의의. 투사층을 통과한 단어들의 벡터는 concatenate하여 다음 은닉층의 input으로 제공
        4. 은닉층을 거치고 거쳐서 출력층에서는 입력과 동일한 차원의 출력이 Softmax를 거쳐 반환
    - "what will the fat cat sit on"에서, "what will the fat cat"을 입력으로 받아 "sit"을 출력하게 만들고 싶다고 가정하자. Window는 4이다.
        1. will, the, fat, cat의 원-핫 인코딩인 [0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0]만 가져온다. -> 이게 헷갈렸던 부분. 4개가 들어간다고 해서 [1, 0, 0, 0], [0, 1, 0, 0], ... , [0, 0, 0, 1]이 아니다.
        2. 투사층 -> 은닉층 -> 출력층을 거쳐 반환되는 벡터는 [p0, p1, p2, p3, p4, p5, p6]인데, 다음 단어가 sit이므로 p5값이 가장 크도록 학습한다.
        2-1. 여기서 '학습'의 대상에는 투사층에서의 가중치 행렬(임베딩 벡터값)도 포함된다.ㄱ
- 결과적으로 수많은 문장에서 유사한 목적으로 사용되는 단어들은(유사한 시퀀스 뒤에 등장하는 단어들은) 유사한 임베딩 벡터값을 얻게된다. -> 결국 같이 쓰이는 단어에 따라 결정되는 것이긴 하네.
Q. 근데, 아까 N-Gram에서 언급한 한계처럼 처음보는 '톺아보다'가 나왔다고 치자. 이게 '살펴보다'와 유사한 의미라는 것, 즉 유사한 임베딩 벡터값을 갖는다는 것은 어떻게 학습시키지?
A. 결국은 톺아보다도 학습해야한다! 다만, 만약 "강아지가 귀엽다." "고양이가 귀엽다."를 N-gram과 word embedding 방식으로 학습시켰다고 하자. 여기서 '강아지가 뛰어놀고 있다.'라는 문장을 추가 학습하게되면, N-gram은 여전히 '고양이가 뛰어놀고 있다.'를 학습하지 않았으므로 그런 문장을 예측할 수 없지만 word embedding에서는 고양이와 강아지 사이에 유사성이 생겼기 때문에 '고양이가 뛰어놀고 있다.'를 예측할 가능성이 있다!
- 문제점! 여전히 고정된 길이의 입력을 받는다!(Window * 전체 단어 개수 크기 만큼만!)