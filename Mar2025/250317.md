# 교재 예시 코드 해설

### NLP 단계별 설명
1. 텍스트를 모두 소문자로 바꾼다. (text.lower())
2. 토큰화(nltk.word_tokenize(text)) 후 불용어 및 특수문자 제거
3. WordNetLemmatizer로 각 단어를 기본형으로 변환(running -> run)
4. tensorflow.keras.preprocessing.text의 Tokenizer를 활용하여 단어 인덱스 구축(tokenizer.fit_on_texts(texts))
    - texts는 preprocess된 문장들이 배열로 존재. 각 문장들 안의 모든 단어를 다 인덱싱 해주는 듯
    - 이 단계에서 모든 단어는 인덱싱되고, 해당 인덱스는 tokenizer.word_index로 확인할 수 있다.
5. 모든 texts들에 패딩을 넣어 같은 길이의 배열로 맞춰준다.
    - tensorflow.keras.preprocessing.pad_sequences를 사용해서 맞춰줄 수 있다. max(len(text) for text in texts)로 최대 길이 구할 수 있음
6. Sequential에 Embedding Layer부터 추가하는데, 임베딩은 각 단어들을 벡터로 바꿔준다.
    - 따라서 input_dim은 단어의 개수(+1. 인덱스 0은 '<pad>'의 index), output_dim은 원하는 단어 벡터의 크기
7. GlobalAveragePooling2D로 output_dim의 평균으로 각 단어들을 표현해준다.
8. 그리고는 학습!