# 데이터 활용 및 구현

### DB?
- DBMS에 기반한 data store. DBMS? 데이터를 캡처하고 분석하는 등 사람 / 앱이랑 소통하는 소프트웨어.
    - 우리가 DB라고 하는 것은 사실상 DBMS
- 작은 데이터베이스는 파일시스템에 저장될 수 잇지만, 큰 규모는 컴퓨터 클러스터나 클라우드에 저장된다.
- 주로 테이블 형식(RDBMS, 주로 SQL 사용) -> 이럴때 테이블 별로 관계를 설정해줘야 하는데 이것이 데이터 베이스 설계. 고급영역! 기술사가 있다
    - 그렇지만 이 분야도 AI가 빠르게 침투하고 있다. 그래도 공부해야 AI가 잘하는지 확인할 수 있다.
- 2000년대 들어서 NoSQL DB도 인기를 얻는다. ex) MongoDB, GraphDB(Neo4j)

### 정규화
- RDBMS에서 이상을 처리하기 위해 테이블을 나눠주는 작업
    - 삽입 이상(Insertion Anomaly) : 원하지 않는 정보까지 입력해야 하는 경우(DB를 너무 안나눴을때)
    - 삭제 이상(Delete Anomaly) : 한 데이터를 삭제할 때 너무 많은 데이터가 삭제되는 경우(이것도 너무 안나눴을때)
    - 갱신 이상(Update Anomaly) : 데이터 갱신할 때 일관성이 없어지는 경우(이건 너무 나눴을 때)
- 이를 해결하기 위해 제 1~5 정규형이 있는데, 실무에서는 2~3 정규형까지 사용하는 것이 보편적이다.
    - 제1정규형 : 각 column은 원자값을 갖는다.(분할할 수 없는 정보를 갖는다.)
    - 제2정규형 : 모든 Column이 Primary Key에만 종속될 때. 즉, PrimaryKey가 아니라 다른 Column 값에 따라 또 다른 Column이 정해져서 안된다는 것.
        - 이건 예시로 보는게 낫다. ex. e-commerce에서 product_name과 product_price를 따로 관리 안하고 구매내역에 다 같이 저장할때.
        - 문제점 : product_price가 변경될 때마다 모든 가격을 다 바꿔줘야한다. + 중복데이터가 많아서 공간낭비!
        - product table과 user table로 나누고 cart table도 추가하면 해결
    - 제3정규형 : Primary Key가 아닌 column에 종속되는 다른 column이 없어야 한다.
        - 2와 차이는 뭐지...?
- 제2정규형과 제3정규형의 차이점!
    - 제2정규형은 "완전 함수 종속"이 되어야 한다.
    - 완전 함수 종속 : 기본 키가 여러 개의 키로 구성되어 있을 때, 기본 키의 일부만 있어도 다른 column의 값이 결정되면 안된다. 즉 종속되면 안된다.
    - 예시 : '이름'과 '수강한 강좌'가 기본키(복합키)라고 했을 때, '학교'는 '이름'만 보고 알 수 있으니 이는 완전 함수 종속에 위배된다.
    - 제3정규형은 제2정규형 + 모든 속성이 기본 키에 "이행적 함수 종속"이 되지 않아야 한다.
    - 이행적 함수 종속 : PRIMARY KEY가 아닌 column 값에 의해 다른 column의 값이 결정되지 않아야 한다.
    - 예시 : '직원ID'가 PRIMARY KEY일때, 같은 Table에 '부서ID' column이 있고 그에 따라 '부서위치'가 결정된다면 이는 이행적 함수 종속에 위배된다.

### SQL
- 구조화된 질의 언어. 데이터를 조회, 추가, 수정, 삭제(CRUD)하기 위해 사용.
- DDL(Data Definition Language), DML(Data Manipulation Language), DCL(Data Control Language)로 분류
- CRUD(Create, Retrieve, Update, Delete)
#### DDL(Data Definition Language)
- CREATE DATABASE/TABLE : 데이터베이스/테이블 생성
- ALTER TABLE : 테이블의 스키마 수정. column 등을 추가(ADD)/삭제(DROP)/수정(MODIFY)
- DROP TABLE : 테이블 삭제
#### DML(Data Manipulation Language) -> CRUD
- INSERT INTO table (*columns) VALUES (*values) : 데이터 삽입(Create)
- SELECT : 데이터 조회(Retrieve)
- UPDATE table SET calumn=value : 데이터 수정(Update)
- DELETE FROM * : 데이터 삭제(Delete)
#### DCL(Data Control Language)
- 사용자에게 권한 부여하고... 뭐 그런 것들. DB에서 눈에 보이지 않는 부분을 다루는 언어인 것 같다.


# 개인 공부

### RNN
#### 진행 과정
1. Hidden Layer의 한 셀(노드)를 기준, 문자 Sequence가 들어올 때 t-1번째 문자를 t-1, t번째 문자를 t라고 하자.
2. t-1이 들어갔을 때 그때의 output이 h_t-1(Hidden State)이 되고, 이는 t가 입력으로 들어올 때 활용된다.
2-1. t는 셀(노드)에 있던 w_x와 곱해지고, h_t-1은 w_h와 곱해진다. 그리고 둘을 더한다.
3. 더한 값에 bias를 또 더해주고, 활성화 함수를 거쳐 다음 층으로 전달된다.
4. 그리고 전달되는 그 값이 또 t+1과 함께 계산된다.
#### 그렇다면 과연 어떻게 학습이 되나?
- 저렇게 Hidden State를 저장하는 이유는, 같은 'work'라도 문맥에 따라 noun/verb가 될 수 있다. -> 문맥을 알아야 한다.
- https://www.youtube.com/watch?v=PahF2hZM6cs 에서는, 각각의 문자를 입력했을 때 해당 문자의 품사를 맞추도록 학습을 시킨다고 한다. -> 이거는 품사태깅에 사용되는 RNN! RNN을 활용하는 다양한 예시 중 하나일 뿐!!(Many-to-Many)
- Sentiment Analysis 에서는 매 단어마다의 뭔가가 중요한 것이 아니고 최종 결과로 나오는 값이 분류에 쓰인다. -> softmax를 가장 마지막 출력에만 적용
    - 나머지 output들은 hidden state에만 쓰이는 듯. -> 맞다(Many-to-one)
- RNN에서 쓰이는 Back Propagation은 이제껏 한것처럼 다른 Layer의 다른 weight들을 뒤로 뒤로 수정해가는 것이 아니라 같은 Layer의 같은 weight(HiddenState)들을 수정해나가는 것 -> Back Propagation through Time.
    - Q. 그럼 매 입력이 들어올때마다 그때그때 계산되는 weight들이 다른가? 같은 weight로 쭉 가는 것이 아니고? -> Keras의 TimeDistributed가 어떤 역할을 하는지 알아보면 될 것 같다. -> X!!! 같은 weight다.
        - https://wikidocs.net/22886 에 따르면 아닌듯! '각각의 가중치의 값은 하나의 층에서는 모든 시점에서 값을 동일하게 공유합니다.'라고 한다. 여기서 '모든 시점에서' 라는걸 보니 같은 weight로 가는게 맞는 것 같다.
        - Back Propagation through Time은 같은 가중치를 시점에 따라 여러번 누적해서 수정한다.
        - 여기서 Chain Rule을 사용하는 Back Propagation의 특성상 입력층에 가까워질수록 기울기 소실이 발생했는데, 여기서는 실제로 입력층에 가까워지는게 아니라 초기 입력에 가까워질수록 기울기 소실이 발생한다.
- 단어를 생성할때는 어떻게 할까? 품사를 맞추는 것이 아니라 다음 단계에 무엇이 올지를 맞추는 건데 어떻게 가능한 것일까.
    - 교재 예제에서 한 번 살펴보면, Input으로는 i ~ i+s 만큼의 단어가 들어가고, label은 i+1 ~ i+s+1이 들어간다.
    - 즉, i가 인풋으로 들어갔을 때 i+1이 나오도록, i+n이 들어갔을 때 i+n+1이 나오도록 학습이 되는 것. -> 이를 i+n이 들어갔을 때, i+n 뿐 아니라 i+n-1이 들어갔을 때의 output값까지 고려해서 i+n+1을 예측해내야 한다는 의미
- Q. RNN의 은닉층은 1개의 개층인가?
    - https://wikidocs.net/22886 에서의 설명에 따르면 여러개의 계층이 있을 수 있다.
    - 설명하는 것을 보면 대체적으로 하나의 계층인 것 같다. Hidden State의 dimension에 따라 표현이 다양해질 수 있음!
- 차원으로 계산해보자. input, 예를 들어 자연어 처리인 경우 단어 벡터의 차원이 d, Hidden State의 크기를 D_h라고 했을 때
    - x_t : (d * 1)
    - W_x : (D_h * d)
    - W_h : (D_h * D_h)
    - h_t : (D_h * 1) -> d차원이었던 Input이 D_h차원으로 변환되고, 그 차원에 따라 쭉 다음 Time Step으로 전달되니까.
- 기억해야할 핵심은, 기존 신경망에서는 출력층에 가까운 Hidden Layer의 Weight부터 차례로 기울기를 계산(Back Propagation)해서 수정해왔다면, 즉 각 Layer마다 서로 다른 Weight를 차례로 수정해왔다면, RNN에서는 (주로) 하나의 Hidden Layer의 Weight를 마지막 Sequence의 Time Step부터 처음 시점까지 기울기를 계산하여(Back Propagation through Time), 하나의 Weight에 누적해서 수정해준다!
    - 그래디언트를 역전파하며 계산하는 시점에는 Weight 수정 X
    - 근데... 이게 그렇게 효과가 괜찮나...? 첫번째 시점에서는 조금만 변화하면 되는데 출력 시점 기울기 변화까지 다 반영해서 한꺼번에 Weight를 수정해버리면...
    - 여기서 Loss는 각 시점에서의 출력 오차를 전부 합친 합이다. 만약 Sentimental Analysis처럼 최종 출력만 사용된다면 Loss는 최종 출력에 대한 오차만 포함. 결국 각 시점에서의 값이 출력 시점에 준만큼의 영향과 그 시점에서의 출력에 대한 모든 책임을 묻는거라고 생각하자.
    - 핵심! 출력 시점에서만 Back Propagation을 하는 것이 아니라 매 시점마다 Back propagation을 진행한다! -> 여기서 W는 공통이라는 것을 잊지 말아야 한다.
    - 간단하게 표현하면, t 시점에서의 그래디언트는 자신의 출력 y_t에 대한 그래디언트와 그 이후의 출력들 (y_t+1, y_t+2, ...)에 대한 그래디언트만큼 계산되어 수정한다.
        - 역전파 될수록 기울기 값이 작아지고(소실), 학습률도 반영되므로 과도하게 수정될 걱정은 안해도 된다.
